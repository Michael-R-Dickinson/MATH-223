\documentclass[12pt, oneside]{amsart}
\usepackage{amsmath,amsfonts, amssymb, xcolor}
\usepackage{fullpage}
\newcommand{\Z}{\mathbb Z}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\spn}{span}

\usepackage[most]{tcolorbox} % powerful colored boxes
\usepackage{xcolor}          % colors
\newtcolorbox{solution}{
  colframe=green!50!black,   % medium green border
  coltext=black,             % body text in black for readability
  boxrule=0.4pt,             % thin, professional border
  enhanced,                  % enable advanced drawing features
  breakable,                 % allow box to break across pages
  left=6pt, right=6pt, top=6pt, bottom=6pt % padding
}

\theoremstyle{definition}
\newtheorem{prob}{Problem}

\title{Math 223 Fall 2025 - Homework 7}
\author{due November4 at 11:59pm}
\pagenumbering{gobble}
\begin{document}
\maketitle
Each problem is worth 10 points.


\begin{prob} Let $A$ be an $n\times n$ matrix with entries in $\mathbb R$.
    \begin{enumerate}
        \item Show that $0\in \mathbb R$ is an eigenvalue of $A$ if and only if $A$ is not invertible.
        
        \begin{solution}
        
        ($\Rightarrow$) Suppose $0$ is an eigenvalue of $A$. By definition, this means there exists a non-zero vector $v \neq 0$ such that $Av = 0$. This tells us there is a non-trivial linear combination of the column vectors of $A$ that equals zero, which means the columns of $A$ are linearly dependent. Therefore $\det A = 0$, so $A$ is not invertible.
        
        ($\Leftarrow$) Suppose $A$ is not invertible. Then $A$ is not injective, which means $\ker(A) \neq \{0\}$. Therefore there exists some $v \neq 0$ such that
        \[
        Av = 0 = 0v
        \]
        This shows that $0$ is an eigenvalue of $A$ with eigenvector $v$.

        \end{solution}
        
        \item Suppose $A$ is invertible. Prove that $\lambda\in \mathbb R$ is an eigenvalue of $A$ if and only if $\lambda^{-1}$ is an eigenvalue of $A^{-1}$.
        
        \begin{solution}
        
        ($\Rightarrow$) As $\lambda$ is an eigenvalue of $A$ there exists a vector $v \neq 0$ such that:
        \[
        Av = \lambda v
        \]

        As $A$ is invertible, we know:
        \[
        A^{-1}A = I
        \]
        We multiply both sides by $v$:
        \[
        (A^{-1}A)v = Iv
        \]

        Because matrix multiplcation is associative, we can rearrange the left side and use our known equation $Av = \lambda v$. Additionally, as $Iv = v$, we simplify the right side as well:

        \[
        A^{-1}(Av) = v
        \]
        \[
        A^{-1}(\lambda v) = v
        \]

        We know that $\lambda \neq 0$ because $A$ is invertible and we proved in part 1 of this problem that $0$ is not an eigenvalue for invertible matrices. Thus we can divide both sides by $\lambda$:
        \[
        A^{-1}v = \frac{1}{\lambda}v = \lambda^{-1}v
        \]
        This shows that $\lambda^{-1}$ is an eigenvalue of $A^{-1}$ with eigenvector $v$.
        
        ($\Leftarrow$) Because $\lambda^{-1}$ is an eigenvalue of $A^{-1}$, there exists a vector $v \neq 0$ satisfying the equation:
        \[
        A^{-1}v = \lambda^{-1}v
        \]
        Multiplying both sides by $A$:
        \[
        A(A^{-1}v) = A(\lambda^{-1}v)
        \]
        \[
        (AA^{-1})v = \lambda^{-1}(Av)
        \]
        \[
        v = \lambda^{-1}(Av)
        \]
        Multiplying both sides by $\lambda$:
        \[
        \lambda v = Av
        \]
        This shows that $\lambda$ is an eigenvalue of $A$ with eigenvector $v$.
        
        \end{solution}
    \end{enumerate}
\end{prob}

\begin{prob} Let $V$ be a vector space over $F$ with $\dim V = n$.
Let $\lambda_1, \dots, \lambda_k$ be eigenvalues of $T:V\to V$. For each $i=1, \dots, k$ let $E_i$ be the eigenspace corresponding to the eigenvalue $\lambda_i$, and let $S_i\subseteq E_i$ be a linearly independent subset of $E_i$. Prove that $S = S_1\cup S_2\cup \dots \cup S_k$ is a linearly independent subset of $V$.
(Hint: Use the fact proven in class that a set of eigenvectors, each corresponding to a distinct eigenvalue, is linearly independent.)

\begin{solution}

Assume for contradiction that $S$ is linearly dependent. This means that there exists a nontrivial linear combination of vectors in $S$ that equals zero: 
\[
\sum_{v} c_v v = 0
\]
\[
\text{with } c_v \in F \text{ and } v \in S
\]

We now define $u_i$ for each $i$ from $1, 2, \dots, k$:
\[
u_i = \sum_{v} c_v v
\]
\[
 \text{with } v \in S_i
\]

We can now rewrite our original equation as:
\[
u_1 + u_2 + \dots + u_k = 0
\]

Since $S_i \subseteq E_i$ and $E_i$ is a subspace (being an eigenspace), $E_i$ is closed under linear combinations. Therefore each $u_i \in E_i$, which means $u_i$ is an eigenvector with eigenvalue $\lambda_i$, or $u_i = 0$.

Now we consider two cases based on whether any values of $u_i$ are nonzero.

\vspace{1em}
Case 1: At least one $u_i \neq 0$.

This means that the linear combination $u_1 + u_2 + \dots + u_m = 0$ has atleast one nonzero term. These non-zero terms, we have established, are eigenvectors corresponding to distinct eigenvalues $\lambda_i$.

Thus the following is a nontrivial linear combination of eigenvectors corresponding to distinct eigenvalues, that equals zero. This contradicts the fact proven in class that eigenvectors corresponding to distinct eigenvalues are linearly independent.

\[
u_1 + u_2 + \dots + u_m = 0
\]


\vspace{1em}
Case 2: All $u_i = 0$.

This means that for each $i$, we have
\[
\sum_{v} c_v v = 0 \text{ for } v \in S_i
\]
But each $S_i$ is linearly independent, so all coefficients $c_v = 0$ for $v \in S_i$. 

This applies to all $i$, and thus all sums $u_i$. Thus, all coefficients in our original linear combination are zero. But this contradicts our assumption that we had a nontrivial linear combination (not all coefficients zero) of vectors $u_i$ equal to 0.

\vspace{1em}
Since both cases lead to contradictions, our assumption that $S$ is linearly dependent must be false. Therefore, $S$ is linearly independent.

\end{solution}
\end{prob}

\begin{prob}
    Consider the following linear operator on the space of polynomials with coefficients in $\mathbb R$ of degree $\leq 2$.
    $T:P_2(\mathbb R) \to P_2(\mathbb R)$
    $T(p) = (2x+1)p' + x^2\cdot p''$
    where $p' = \frac{dp}{dx}$ and $p'' = \frac{d^2p}{dx^2}$. Find all the eigenvalues of $T$, and for each eigenvalue find an eigenvector.
    
    \begin{solution}
    
    We'll find a matrix representation of $T$ with the basis $\{1, x, x^2\}$ for $P_2(\mathbb{R})$.

First, we apply $T$ to each basis element:

For $T(1)$:
\[
T(1) = (2x+1) \frac{d}{dx}(1) + x^2 \frac{d^2}{dx^2}(1) = 0
\]

For $T(x)$:
\[
T(x) = (2x+1) \frac{d}{dx}(x) + x^2 \frac{d^2}{dx^2}(x) = (2x+1)(1) + 0 = 2x + 1
\]

For $T(x^2)$:
\[
T(x^2) = (2x+1) \frac{d}{dx}(x^2) + x^2 \frac{d^2}{dx^2}(x^2) = (2x+1)(2x) + x^2(2) = 4x^2 + 2x + 2x^2 = 6x^2 + 2x
\]

Writing these in terms of the basis $\{1, x, x^2\}$, we have:
\begin{itemize}
    \item $T(1) = 0 \cdot 1 + 0 \cdot x + 0 \cdot x^2$
    \item $T(x) = 1 \cdot 1 + 2 \cdot x + 0 \cdot x^2$
    \item $T(x^2) = 0 \cdot 1 + 2 \cdot x + 6 \cdot x^2$
\end{itemize}

Therefore, the matrix representation of $T$ is:
\[
A = \begin{bmatrix} 0 & 1 & 0 \\ 0 & 2 & 2 \\ 0 & 0 & 6 \end{bmatrix}
\]

To find the eigenvalues, we compute the characteristic polynomial by solving for $\lambda$ in $\det(A - \lambda I) = 0$:
\[
\det(A - \lambda I) = \det \begin{bmatrix} -\lambda & 1 & 0 \\ 0 & 2-\lambda & 2 \\ 0 & 0 & 6-\lambda \end{bmatrix}
\]

Computing the determinant using cofactor expansion along the first row:
\[
(-\lambda) \det \begin{bmatrix} 2-\lambda & 2 \\ 0 & 6-\lambda \end{bmatrix} - 1 \det \begin{bmatrix} 0 & 2 \\ 0 & 6-\lambda \end{bmatrix} + 0
\]
\[
= (-\lambda)[(2-\lambda)(6-\lambda)] - 1[0] = -\lambda(\lambda^2 - 8\lambda + 12)
\]
\[
= -\lambda^3 + 8\lambda^2 - 12\lambda
\]

Factoring out $\lambda$:
\[
\lambda(\lambda^2 - 8\lambda + 12) = 0
\]
\[
\lambda(\lambda - 6)(\lambda - 2) = 0
\]

This gives us eigenvalues $\lambda = 0, 2, 6$.

Now we find an eigenvector for each eigenvalue.

For $\lambda = 0$, we solve $(A - 0I)v = 0$:
\[
\begin{bmatrix} 0 & 1 & 0 \\ 0 & 2 & 2 \\ 0 & 0 & 6 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}
\]

From the third row: $6v_3 = 0$, so $v_3 = 0$.\\
From the second row: $2v_2 + 2v_3 = 0$, so $v_2 = 0$.\\
The first row is automatically satisfied.\\
Taking $v_1 = 1$, we get eigenvector $v = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$, which corresponds to $p(x) = 1$.

For $\lambda = 2$, we solve $(A - 2I)v = 0$:
\[
\begin{bmatrix} -2 & 1 & 0 \\ 0 & 0 & 2 \\ 0 & 0 & 4 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}
\]

From the second and third rows: $v_3 = 0$.\\
From the first row: $-2v_1 + v_2 = 0$, so $v_2 = 2v_1$.\\
Taking $v_1 = 1$, we get eigenvector $v = \begin{bmatrix} 1 \\ 2 \\ 0 \end{bmatrix}$, which corresponds to $p(x) = 1 + 2x$.

For $\lambda = 6$, we solve $(A - 6I)v = 0$:
\[
\begin{bmatrix} -6 & 1 & 0 \\ 0 & -4 & 2 \\ 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}
\]

From the second row: $-4v_2 + 2v_3 = 0$, so $v_2 = \frac{v_3}{2}$.\\
From the first row: $-6v_1 + v_2 = 0$, so $v_1 = \frac{v_2}{6} = \frac{v_3}{12}$.\\
Taking $v_3 = 12$, we get $v_2 = 6$ and $v_1 = 1$, giving eigenvector $v = \begin{bmatrix} 1 \\ 6 \\ 12 \end{bmatrix}$, which corresponds to $p(x) = 1 + 6x + 12x^2$.

Finally, we have found eigenvalues $\lambda = 0, 2, 6$ with corresponding eigenvectors $p(x) = 1$, $p(x) = 1 + 2x$, and $p(x) = 1 + 6x + 12x^2$.
    
    \end{solution}
\end{prob}

\begin{prob}
    Determine if the given matrix is diagonalizable. If it is diagonalizable what is the diagonal matrix that is similar to it? Show all your work.
    \begin{enumerate}
        \item $A_1 = \left[\begin{matrix} 1 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 3\end{matrix}\right]$
        
        \begin{solution}
        
        $A_1$ is upper triangular so we can read the eigenvalues off the diagonal: $\lambda = 1$ and $\lambda = 3$.

Now we find the eigenvectors for each eigenvalue.

{For $\lambda = 1$:} We solve $(A - I)v = 0$:
\[
\begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 2 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}
\]

We can write this as a system of equations and solve th get the eigenvectors for this eigenvalue:

\begin{align*}
v_2 &= 0 \\
2v_3 &= 0
\end{align*}

The value $v_1$ is a free variable as it is none of the equations. We can pick $v_1 = 1$ and solve for $v2$ and $v_3$ to get the eigenvector:
\[v = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}\]

{For $\lambda = 3$:} We solve $(A_1 - 3I)v = 0$:
\[
\begin{bmatrix} -2 & 1 & 0 \\ 0 & -2 & 0 \\ 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}
\]

We get a similar system of equations to the first part and solve for the free variables to get the eigenvector for this eigenvalue:

\begin{align*}
-2v_1 + v_2 &= 0 \\
-2v_2 &= 0
\end{align*}

As $v_3$ is free we can pick it and solve for another eigenvalue. Pick $v_3 = 1$ gets us eigenvector $v = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$.

Thus the eigenvectors for $A$ are $\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$ and $\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$. 
The span of these two eigenvectors is 2-dimensional, which is less than the 3 dimensions we need for a basis of $\mathbb{R}^3$ (the space for $A_1$). Since the eigenvectors do not form a basis for $\mathbb{R}^3$, $A_1$ is not diagonalizable.
        
        \end{solution}

        \item $A_2 = \left[\begin{matrix} 3 & 1 & 1 \\ 2 & 4 & 2 \\ -1 & -1 & 1\end{matrix}\right]$
        
        \begin{solution}
        
        First, we find the eigenvalues by computing the characteristic polynomial.
        \[
        P_{A}(\lambda) 
        = \det(A_2 - \lambda I)
        = \det(\begin{bmatrix} 3-\lambda & 1 & 1 \\ 2 & 4-\lambda & 2 \\ -1 & -1 & 1-\lambda\end{bmatrix})
        \]
        
        We take the determinant by using the cofactor expansion and simplifying:
        \[
        \det(A_2 - \lambda I) = (3-\lambda) \det\begin{bmatrix} 4-\lambda & 2 \\ -1 & 1-\lambda\end{bmatrix} - 1 \cdot \det\begin{bmatrix} 2 & 2 \\ -1 & 1-\lambda\end{bmatrix} + 1 \cdot \det\begin{bmatrix} 2 & 4-\lambda \\ -1 & -1\end{bmatrix}
        \]
        
        \[
        = -\lambda^3 + 8\lambda^2 - 20\lambda + 16 = -(\lambda^3 - 8\lambda^2 + 20\lambda - 16)
        \]
        
        We factor this cubic polynomial to find the eigenvalues by testing values and checking if they factor with synthetic division. This results in finding that $\lambda = 2$ is a root. Finally we can factor the polynomial as:
        \[
        = -(\lambda - 2)^2(\lambda - 4)
        \]
        
        The roots of the characteristic polynomial are the eigenvalues of the matrix. So the eigenvalues of $A_2$ are $\lambda = 2$ and $\lambda = 4$.
        
        Now we find the eigenvectors for each eigenvalue by solving the equation $(A-\lambda I)v =0$.
        
        {For $\lambda = 2$} we solve $(A_2 - 2I)v = 0$:
        \[
        \begin{bmatrix} 1 & 1 & 1 \\ 2 & 2 & 2 \\ -1 & -1 & -1\end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}
        \]
        
        Row reducing to solve results in the matrix:
        \[
        \begin{bmatrix} 1 & 1 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}
        \]
        
        
        We have two free variables ($v_2$ and $v_3$), so we can pick values for each and find two linearly independent eigenvectors:
        
        \[
        v_1= \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix} \quad v_2 = \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix}
        \]
        
        {For $\lambda = 4$} we solve $(A_2 - 4I)v = 0$:
        \[
        \begin{bmatrix} -1 & 1 & 1 \\ 2 & 0 & 2 \\ -1 & -1 & -3\end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}
        \]
        
        Row reducing with gaussian elimination:
        \[
        \begin{bmatrix} -1 & 1 & 1 \\ 0 & 2 & 4 \\ 0 & 0 & 0 \end{bmatrix}
        \]
        
        We can now solve for the eigenvector by making a system of equations from the rows:
        \begin{align*}
        2v_2 + 4v_3 &= 0 \\
        -v_1 + v_2 + v_3 &= 0
        \end{align*}
        We have one free variable, so we can pick a value for it and find an eigenvector:
        We pick the value $v_3 = 1$ and get the eigenvector $v = \begin{bmatrix} -1 \\ -2 \\ 1 \end{bmatrix}$

        \vspace{1em}
        
        We have found three eigenvectors: two from the eigenspace for $\lambda = 2$ and one from the eigenspace for $\lambda = 4$. 
        
        The two eigenvectors from $\lambda = 2$ we already showed are linearly independent.
        Additionally, the third eigenvector is also linearly independent of the first two because it corresponds to a different eigenvalue ($\lambda = 4$) and we know that eigenvectors corresponding to distinct eigenvalues are linearly independent.
        
        Thus, we have three linearly independent eigenvectors in $\mathbb{R}^3$ so they can form a basis for it. Therefore as the space of $A_2$ is $\mathbb{R}^3$, we know that it is diagonalizable.
        

        The diagonal matrix similar to $A_2$ is:
        \[
        D = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 4 \end{bmatrix}
        \]
        
        \end{solution}
    \end{enumerate}
\end{prob}

\noindent \textbf{Generative AI Acknowledgment}

\noindent Generative AI was used in this assignment for help with latex syntax. This includes defining the "solution box" that wraps answers. This also includes transcribing verbatum from handwritten on-paper solutions to first-draft latex code.
It was also used to check answers before submission.

\end{document}