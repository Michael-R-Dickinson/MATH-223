\documentclass[12pt, oneside]{amsart}
\usepackage{amsmath,amsfonts, amssymb, xcolor}
\usepackage{fullpage}

\usepackage[most]{tcolorbox} % powerful colored boxes
\usepackage{xcolor}          % colors
\newtcolorbox{solution}{
  breakable,              % allow page breaks
  enhanced,               % better control of title etc.
  colback=white,          % white background
  colframe=gray,          % solid gray border
  boxrule=0.5pt,          % border thickness
  left=6pt, right=6pt, top=6pt, bottom=6pt, % padding
  title=\textbf{Solution},
  coltitle=black,
  fonttitle=\normalsize\bfseries,
  attach boxed title to top left={yshift=-2pt, xshift=4pt},
  boxed title style={
    colback=white,
    colframe=white,
    boxrule=0pt,
    sharp corners
  }
}

\newcommand{\Z}{\mathbb Z}
\newcommand{\R}{\mathbb R}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\proj}{proj}
\newcommand\norm[1]{\left\lVert#1\right\rVert}

\theoremstyle{definition}
\newtheorem{prob}{Problem}

\title{Math 223 Fall 2025 - Homework 10}
\author{due December 4 at 11:59pm}
\pagenumbering{gobble}
\begin{document}
\maketitle
Each problem is worth 10 points.

\begin{prob}
  A map $T: V\to W$ is an isometry if $\norm{S(v)} = \norm{v}$ for all $v\in V$. Show that the following conditions are equivalent:
  \begin{enumerate}
    \item $S$ is an isometry

      \begin{solution}
        (1) $\implies$ (2):

        To show that $S^*S(v) = v$ for all $v \in V$ we can show that the inner product of $S^*S(v)$ with any vector $w \in V$ equals the inner product of $v$ with $w$:

        \[
          \langle S^*S(v), w \rangle = \langle v, w \rangle
        \]
        \[
          v, w \in V
        \]

        \vspace{0.3cm}

        First, we use the adjoint to rewrite the left side:
        \[
          \langle S^*S(v), w \rangle = \langle S(v), S(w) \rangle.
        \]

        \vspace{0.3cm}

        We know $\norm{S(u)}^2 = \norm{u}^2$ for any vector $u$. We can apply this to $u = v + w$:
        \[
          \norm{S(v + w)}^2 = \norm{v + w}^2.
        \]
        Now, we can expand both sides using the properties of inner products and rearrange to get both sides of our desired result.

        First, the left hand side:

        Since $S$ is linear we can expand out the left side:
        \[
          \norm{v + w}^2 =
          \norm{S(v) + S(w)}^2
        \]

        Then using the definition of the norm as an inner product we can expand this further, and use the fact that $S$ is an isometry to simplify $\norm{S(v)}$ and $\norm{S(w)}$:
        \[
          \norm{S(v)}^2 + 2\langle S(v), S(w) \rangle + \norm{S(w)}^2 = \norm{v}^2 + 2\langle S(v), S(w) \rangle + \norm{w}^2
        \]

        Now for the right hand side we can similarly expand using the definition of the norm as an inner product:
        \[
          \norm{v + w}^2 = \norm{v}^2 + 2\langle v, w \rangle + \norm{w}^2.
        \]

        Setting the two sides equal to eachother we now have the equation:
        \[
          \norm{v}^2 + 2\langle S(v), S(w) \rangle + \norm{w}^2
          = \norm{v}^2 + 2\langle v, w \rangle + \norm{w}^2.
        \]
        We can cancel $\norm{v}^2$ and $\norm{w}^2$ from both sides and divide by 2 to get:
        \[
          \langle S(v), S(w) \rangle = \langle v, w \rangle.
        \]

        which is what we wanted to prove.
      \end{solution}

    \item $S^* S = \id_V$
    \item $\langle S(v)\mid S(v')\rangle = \langle v \mid v' \rangle$ for all $v,v'\in V$\\
  \end{enumerate}
\end{prob}

\begin{prob}
  Let $T:V\to W$ be a linear map between finite dimensional inner product spaces. Let $\mathcal B = (v_1, \dots, v_n), \mathcal C = (w_1, \dots, w_m)$ be orthonormal bases of $V,W$ respectively. Prove that
  $$\norm{T(v_1)}^2 +\dots +\norm{T(v_n)}^2 = \norm{T^*(w_1)}^2+ \dots + \norm{T^*(w_m)}^2.$$\\
\end{prob}

\begin{solution}
  Let $A$ be the $m \times n$ matrix of $T$ with respect to the orthonormal bases $\mathcal{B}$ and $\mathcal{C}$.

  \vspace{0.3cm}

  For the left-hand side, by definition, the $j$-th column of $A$ is the result of $T(v_j)$ because $v_j$ is in the basis of the domain of $T$, expressed as coordinates in the basis $\mathcal{C}$. Thus we write $T(v_j)$ as the linear combination of the basis vectors in $\mathcal{C}$:

  \[
    T(v_j) = A_{1j}w_1 + A_{2j}w_2 + \cdots + A_{mj}w_m.
  \]
  Since $\mathcal{C}$ is orthonormal, we can compute the norm of $T(v_j)$ from these coordinates (shown in class that we can do this only if the basis is orthonormal - probably):
  \[
    \norm{T(v_j)}^2 = |A_{1j}|^2 + |A_{2j}|^2 + \cdots + |A_{mj}|^2.
  \]
  Now we can do this for all basis vector $v_1, \ldots, v_n$ to get the left-hand side of our equation:
  \[
    \norm{T(v_1)}^2 + \cdots + \norm{T(v_n)}^2 = \sum_{i,j} |A_{ij}|^2.
  \]

  \vspace{0.3cm}

  For the right-hand side, the matrix of $T^*$ with respect to $\mathcal{C}$ and $\mathcal{B}$ is $A^*$. The $i$-th column of $A^*$ is the conjugate of the $i$-th row of $A$, so:
  \[
    T^*(w_i) = \overline{A_{i1}}v_1 + \overline{A_{i2}}v_2 + \cdots + \overline{A_{in}}v_n.
  \]
  Since $\mathcal{B}$ is orthonormal, we can compute the norm directly from these coordinates and then sum over all basis vectors $w_1, \ldots, w_m$ with the same logic as we used for the left-hand side:
  \[
    \norm{T^*(w_i)}^2 = |\overline{A_{i1}}|^2 + |\overline{A_{i2}}|^2 + \cdots + |\overline{A_{in}}|^2 = |A_{i1}|^2 + |A_{i2}|^2 + \cdots + |A_{in}|^2.
  \]
  Summing all basis vectors $w_1, \ldots, w_m$:
  \[
    \norm{T^*(w_1)}^2 + \cdots + \norm{T^*(w_m)}^2 = \sum_{i,j} |A_{ij}|^2.
  \]

  As we have shown that both sides equal to $\sum_{i,j} |A_{ij}|^2$, we conclude that:
  \[
    \norm{T(v_1)}^2 + \cdots + \norm{T(v_n)}^2 = \norm{T^*(w_1)}^2 + \cdots + \norm{T^*(w_m)}^2.
  \]

\end{solution}

\begin{prob}
  Which of these matrices are similar? Justify.
  $$A = \left[
    \begin{matrix} 1& 0 &1& 0\\0& 1&0&1\\1&0&1&0\\0&1&0&1
  \end{matrix}\right] \qquad
  B= \left[
    \begin{matrix} 2& 1 &0& 0\\0& 2&0&0\\0&0&0&0\\0&0&1&0
  \end{matrix}\right]\qquad
  C= \left[
    \begin{matrix} 2& 0 &0& 0\\0& 0&0&0\\0&0&0&0\\0&0&0&2
  \end{matrix}\right] \qquad
  D = \left[
    \begin{matrix} 1& 1 &0& 0\\1& 1&0&0\\0&0&1&1\\0&0&1&1
  \end{matrix}\right]
  $$ \\
\end{prob}

\begin{prob}
  Find $\det A$ using the eigenvalues of $A$, where $$A = \left[
    \begin{matrix} 7& 2 &2& 2& 2\\2& 7&2&2&2\\2&2&7&2&2\\2&2&2&7&2 \\ 2&2&2&2&7
  \end{matrix}\right].$$
  (Hint: there exists $\lambda$ such that the rows of $(A - \lambda\cdot I)$ sum up to $0$.)
\end{prob}
\end{document}